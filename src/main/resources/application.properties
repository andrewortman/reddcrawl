http.port=${HTTP_PORT:8085}

#runs on postgres with hibernate (will probably switch to jdbc later, though)
db.driver=org.postgresql.Driver
db.url=jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_DATABASE:reddcrawl}
db.username=${DB_USERNAME:postgres}
db.password=${DB_PASSWORD:postgres}
db.initialSize=${DB_POOL_INITIAL_SIZE:12}
db.maxSize=${DB_POOL_MAX_SIZE:32}
db.minIdle=${DB_POOL_MIN_IDLE:12}
db.hibernate.dialect=org.hibernate.dialect.PostgreSQL9Dialect
db.hibernate.hbm2ddl.auto=none

#client configuration
client.endpoint=https://oauth.reddit.com
client.useragent=${REDDCRAWL_USERAGENT:script:reddcrawl:v0.2 (by /u/reddcrawl)}
client.timeout.connect=${REDDCRAWL_CLIENT_CONNECT_TIMEOUT:5000}
client.timeout.read=${REDDCRAWL_CLIENT_READ_TIMEOUT:10000}
client.rpm=${REDDCRAWL_REQ_PER_MINUTE:60}

#client oauth authentication (required)
client.oauth.endpoint=https://www.reddit.com
client.oauth.username=${REDDCRAWL_USERNAME}
client.oauth.password=${REDDCRAWL_PASSWORD}
client.oauth.clientid=${REDDCRAWL_CLIENTID}
client.oauth.clientsecret=${REDDCRAWL_SECRET}

#file archiver
service.archive.file.directory=${REDDCRAWL_SERVICE_ARCHIVE_DIRECTORY:/tmp}
#google archiver (set $GOOGLE_APPLICATION_CREDENTIALS to the location of the google api creds to use this instead of the file archiver)
service.archive.google.bucket=${REDDCRAWL_GOOGLE_BUCKET:reddcrawl}

#when a story should be archived to disk
service.archive.oldeststory=${REDDCRAWL_SERVICE_ARCHIVE_OLDEST_STORY_AGE:172800}
#number of seconds to wait in between batches (increase this number in order to reduce load on postgres during big purges)
service.archive.batchinterval=${REDDCRAWL_SERVICE_ARCHIVE_BATCH_INTERVAL:5}
#max number of stories per batch (reduce number to reduce load on postgres during big purges)
service.archive.maxbatchsize=${REDDCRAWL_SERVICE_ARCHIVE_MAX_BATCH_SIZE:100}

#number of stories in the /new feed to scrape
service.newstoryscraper.newstorycount=${REDDCRAWL_STORYSCAVENGER_COUNT:400}
#number of stories in the /hot feed to scrape
service.newstoryscraper.hotstorycount=${REDDCRAWL_STORYSCAVENGER_COUNT:400}
#time before we consider a subreddit no longer part of the default subreddits
service.newstoryscraper.subredditexpirationinterval=${REDDCRAWL_STORYSCAVENGER_SUBREDDITEXPIRATIONINTERVAL:10800}
#interval between scraper calls (seconds)
service.newstoryscraper.interval=${REDDCRAWL_STORYSCAVENGER_INTERVAL:60}

#oldest age of any story being tracked
service.storyhistoryupdater.oldeststory=${REDDCRAWL_STORYHISTORYUPDATER_OLDEST_STORY_AGE:172800}
#number of seconds between history updater calls (higher number = more stories tracked at once)
service.storyhistoryupdater.interval=${REDDCRAWL_STORYHISTORYUPDATER_INTERVAL:120}
#number of concurrent api calls made during each history update invocation (to avoid a timeout from wasting overall time)
service.storyhistoryupdater.workers=${REDDCRAWL_STORYHISTORYUPDATER_WORKERS:4}
#number of seconds between subreddit history update calls (this is an expensive operation, so only do it once and a while)
service.subreddithistoryupdater.interval=${REDDCRAWL_SUBREDDITHISTORYUPDATER_INTERVAL:1800}

#datadog support
#leave apikey empty to disable
metrics.datadog.apikey=${DATADOG_APIKEY:}
#seconds between stat purges
metrics.datadog.interval=${DATADOG_INTERVAL:5}

#signalfx support
#leave apikey empty to disable
metrics.signalfx.apikey=${SIGNALFX_APIKEY:}
#seconds between stat purges
metrics.signalfx.interval=${SIGNALFX_INTERVAL:5}
